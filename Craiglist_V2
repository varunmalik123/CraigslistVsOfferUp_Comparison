"""
Created on Fri Oct 25 14:53:18 2019

@author: kewang
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import multiprocessing

def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return 'Exception!'
    
def nextPage(soup):
    '''
    return: next_page url
    '''    
    url = 'https://sandiego.craigslist.org'
    body = soup.body
    button = body.section.find('span', class_ = 'buttons')
    buttonnext = button.find('a', class_ = 'button next')

    url +=  buttonnext['href']

    return url
    
def singlePost(url):
    '''
    param: url
    return: list of attributes list[str]
    
    '''
    r = getHTMLText(url)
    soup = BeautifulSoup(r,'html.parser')
    section= soup.body.section.section.section
    attrs = section.find('p', class_ = 'attrgroup')
    
    res = []
    try: 
        for a in attrs.find_all('span'):
            attr = ''
            for s in a:
                attr += s.string
            res.append(attr)
    except:
        pass
    postID = section.find('p', class_ = 'postinginfo').string
    
    return res, postID

def fillItemList(text):
    '''
    param: text
    return: soup, list[list]
    
    
    '''
    soup = BeautifulSoup(text,'html.parser')
    body = soup.body
    for child in body.section.children:
        if child.name == 'form':
            form = child
        
    ct = form.find('div', class_ = 'content')
    item_list = ct.ul
    item_info = []

    for item in item_list.find_all('li'):
    
        item_title = item.p.a.string
        
        meta = item.p.find('span', class_ = 'result-meta')
        item_price = meta.span.string
        post_date = item.p.find('time')
        item_date = post_date['datetime']
        item_url = item.a['href']
        
        attr, postID = singlePost(item_url) #attr = list[str], postID = str
        
        image = 'No' if len(item.a['class']) == 3 else 'Yes'
        name = item_title
        price = item_price
        postDate = item_date
        
        item_info.append([name, price, postDate, image, attr, postID])
        
    return soup, item_info

def outputFile(d, path):
    '''
    param: d
    type: list[list]
    
    '''
    name, price, date, image, attr, postID = [],[],[],[],[], []
    for i in d:
        name.append(i[0])
        price.append(i[1])
        date.append(i[2])
        image.append(i[3])
        attr.append(i[4])
        postID.append(i[5])
    dataframe = pd.DataFrame({'Name': name, 'Price': price, 'postDate': date, 'HasImage': image, 'Attributes': attr, 'postID': postID})
    dataframe.to_csv(path, index = True) 

 
def start(url):
    
    r = getHTMLText(url)
    time = datetime.datetime.now()
    out_path= '/Users/kewang/Desktop/ECE143/test_{}_{}.csv'.format(url[49:57], time) #url[49:57] get product name in url
    
    print('now processing page 1')
    s, d = fillItemList(r) #process first page
    
   
    try: 
        for i in range(10):
            print('now processing page {}'.format(i + 2))
            url = nextPage(s)
            #print(url)
            r = getHTMLText(url)
            s_next, d_next = fillItemList(r)
            for i in d_next:
                d.append(i)
            s = s_next
    except:
        pass

    outputFile(d, out_path)


if __name__ == '__main__':
    
    i = datetime.datetime.now()
    while True:
        
        urls = []
        url_iphonex = 'https://sandiego.craigslist.org/search/sss?query=iphone+x&sort=rel'
        url_iphone8 = 'https://sandiego.craigslist.org/search/sss?query=iphone+8&sort=rel'
        urls.append(url_iphonex)
        urls.append(url_iphone8)
       
        pool = multiprocessing.Pool(multiprocessing.cpu_count())
        pool.map(start, urls) #scrape differnt urls

        pool.close()
        pool.join()
       
        print('finish round {}'.format(i))
        time.sleep(120) # scrape after 120s
        i = datetime.datetime.now()
        
    
    
    
   
